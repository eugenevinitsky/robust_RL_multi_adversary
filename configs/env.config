[env]
time_limit = 50
time_step = 0.25
val_size = 100
test_size = 500
randomize_attributes = false
# How many grid cells there are in x and whether
discretization = 162
# Whether to add gaussian noise to the observed states
add_gaussian_noise_state = false
# Scaling coefficient on gaussian noise
gaussian_noise_state_stddev = 0.1
# Whether to add gaussian noise to the actions
add_gaussian_noise_action = false
# Scaling coefficient on gaussian action noise
gaussian_noise_action_stddev = 0.1


[reward]
success_reward = 10
#reward for reaching goal
collision_penalty = -0.25
# penalty for colliding with humans
discomfort_dist = 0
# distance where robot is too close to human (adopted from CrowdNav)
discomfort_penalty_factor = 0
# penalty for being within discomfort_dist of human (adopted from CrowdNav)
edge_discomfort_dist = 0.3
# if robot is within edge_discomfort_dist of the edge of the accessible space, edge_penalty applied
edge_penalty = -.1
closer_goal = .1
# reward for moving closer to the goal in a step


[sim]
train_val_sim = circle_crossing
test_sim = circle_crossing
square_width = 10
# width of square for 'square_crossing' behaviour (adopted from CrowdNav)
circle_radius = 3
# radius of circle for 'circle_crossing' behaviour (adopted from CrowdNav)
accessible_space = 4
# accessible space robot can navigate through is x \in [-accessible_space, accessible_space], y \in [-accessible_space, accessible_space]
goal_region = 3
# goals can be generated within x \in [-goal_region, goal_region], y \in [-goal_region, goal_region]
randomize_goals = true
# if true, then goals will be randomly generated
update_goals = true
# if true, then a new goal will be generated every time a robot reaches it. else, done when goal is reached
human_num = 1


[humans]
visible = true
policy = orca
radius = 0.3
v_pref = 0.5
sensor = coordinates
chase_robot = true
# if true, then human default goal will be robot's next location. Else, default human behaviour (circle_crossing)


[robot]
visible = true
policy = none
radius = 0.3
v_pref = 1
sensor = coordinates


[train_details]
# This is how many timesteps worth of frames are input to the policy
num_stacked_frames = 2
# This is the x and y limits of the grid size
grid_limit = 6


[transfer]
# If first_step the colors will be swapped every reset, if every_step the colors will be swapped every call to step
change_colors_mode = no_change
# If true, the actions that are actually passed to the robot will be slightly lower than the actions output by the policy
friction = false
# This is a scaling factor on how much less the actions are if friction is true. Check agent.py for more details.
friction_coef = 0.2
